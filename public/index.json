[{"categories":null,"contents":"Addressed pretty significant page load performance issue founde in larger deployments. Eliminates uses of intensive backend query, replacing it with an asynchronous API call against a lucene index. This change reduces page load from from 2+ minutes to nearly instant, with an incredibly responsive UI.\n","permalink":"https://michaeltrip.nl/projects/contributions/deploy-triggers/","tags":["Java","jQuery","REST APIs","Bamboo","JSON"],"title":"Atlassian Deployment Triggers"},{"categories":null,"contents":"This talk looked at Liberty Mutual’s transformation to Continuous Integration, Continuous Delivery, and DevOps. For a large, heavily regulated industry, this task can not only be daunting, but viewed by many as impossible. Often, organizations try to reduce the friction through micro-fixes, but Eddie’s team asked how to change the culture to reduce the friction and concluded with the following final points:\nDon’t mandate DevOps. Give employees the chance to master their discipline with examples to set and follow. Favor deep end-to-end accomplishments over broad but incremental steps forward. Focus on taking the right teams far before encouraging broad adoption. Centralize the platforms and tools that your teams shouldn’t be thinking about. Provide foundational services/commodities and let teams stay on purpose. Incorporate contributions from everyone; don’t stifle autonomy. Stay open to new ways of working. Challenge security policies, but respect intentions. Find new ways to enforce concerns without abandoning precaution. ","permalink":"https://michaeltrip.nl/publications/alldaydevops/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","agile","Culture"],"title":"Organically DevOps: Building Quality and Security into the Software Supply Chain at Liberty Mutual"},{"categories":null,"contents":"Shields.io is a massive library of badges that can be inserted into project README\u0026rsquo;s or websites displaying various statuses (code coverage, health, version, etc). Support for docker was missing the current build health, and was a pretty trivial addition.\n","permalink":"https://michaeltrip.nl/projects/contributions/shields-docker/","tags":["Docker","Rest APIs","JavaScript","node.js","JSON"],"title":"Added Docker Build Status Badge to shields.io"},{"categories":null,"contents":"While adding Structured Data to a client\u0026rsquo;s website I found some example JSON that was invalid. Simple contribution to cleanup the user documentation providing syntactically valid JSON documents.\n","permalink":"https://michaeltrip.nl/projects/contributions/schema-org/","tags":["JSON"],"title":"Schema.org Structured Data documentation fixes"},{"categories":null,"contents":"Intro After trying to create a SNO Openshift Node in my Homelab, it was time to scale it up even further and trying to create a OpenShift cluster in the cloud. For this use case i will be using GCP.\nPreperations First, there are some preperations to be done;\nA service account in GCP with a lot of permissions in GCP Enable certain API\u0026rsquo;s in GCP A Managed DNS zone in GCP Both the oc binary and the openshift-install binary. Create a directory structure. Create a Service account in GCP Go to the Google Cloud Console; https://cloud.google.com and log-in there. Go to your favorite project, mine is called michaeltcloud. Go to the IAM \u0026amp; Admin menu en choose ServiceAccounts. Click on the button \u0026ldquo;Create Service Account\u0026rdquo; Fill in the name and click on \u0026ldquo;Create and Continue\u0026rdquo;. After that, you can grant roles to the newly created service account. This is the official list from Red Hat\nRequired roles during installation:\nCompute Admin Security Admin Service Account Admin Service Account User Storage Admin DNS Administrator Service Account Key Admin After selecting all roles your service account is ready.\nClick on Done. After that, go to the service account page and click on the three-dots next to the newly created service account and click on Manage keys Click on Add key and choose Create New Key. Choose JSON and download the newly created json file and store it somewhere safe. Afer that, create a directory in your home directory called .gcp and place this file in the directory with the name osServiceAccount.json.\nNOTE: The installer will also create 2 new service accounts so the nodes can use them to interact with the google cloud API. The Roles for each type of service account:\nEnabling APIs For the installer to work properly, we need to enable certain APIs. This is the table of APIs that need to be enabled.\nAPI service Console service name Compute Engine API compute.googleapis.com Google Cloud APIs cloudapis.googleapis.com Cloud Resource Manager API cloudresourcemanager.googleapis.com Google DNS API dns.googleapis.com IAM Service Account Credentials API iamcredentials.googleapis.com Identity and Access Management (IAM) API iam.googleapis.com Service Management API servicemanagement.googleapis.com Service Usage API serviceusage.googleapis.com Google Cloud Storage JSON API storage-api.googleapis.com Cloud Storage storage-component.googleapis.com This can easily be done by using the gcloud cli tool.\n[michael@michael-nb2 ~]$ gcloud services enable compute.googleapis.com Run this command for every api and you are done.\nCreating a managed DNS zone as mentioned before, OpenShift heavily relies on DNS. So it is key to have DNS zone in your GCP project.\nBut how do you do that? If you have a spare domain, you can add it entirely to GCP. But you can also designate a subdomain to GCP. That is what i have done. I have a dedicated sub domain zone called gcp.alcatrash.net. The only thing you need to do is create a few NS records at your DNS provider and you are good to go.\nThese are my NS records for example:\nAfter that, you can create a DNS zone in GCP. Go to your project, search for Cloud DNS. and click on Create Zone.\nThe zone name will be your sub domain, the DNS Name will be your base domain. After that, click on create.\nDownloading the installer The installer is available at the Releases page on the Github of the OKD Project. https://github.com/okd-project/okd/releases\nIn this demo we will download the 4.11.0-0.okd-2022-08-20-022919 version. Scroll down to the assets and copy the links for both the openshift-client and the openshift-install and do a wget for those files. After that, extract them somewhere in a directory in your $PATH.\nCreate a directory structure Now, create a directory in your home directory called okd_deploy and a sub directory called install_dir.\n[michael@michael-nb2 ~]$ mkdir -p okd_deploy/install_dir [michael@michael-nb2 ~]$ cd okd_deploy/ [michael@michael-nb2 okd_deploy]$ tree . └── install_dir After that, you are all set. The only thing we need to do is configure the yaml file that will contain all the information for the openshift-install to do its work\nThe YAML file that will do the magic The yaml file called install-config.yaml can be either generated by running the openshift-install program interactively. After that, you can edit it to fit your own needs. For now, i will provide the install-config.yaml for you so you don\u0026rsquo;t have to go into the interactive setup mode.\napiVersion: v1 baseDomain: gcp.alcatrash.net compute: - architecture: amd64 hyperthreading: Enabled name: worker platform: gcp: osDisk: diskType: pd-ssd diskSizeGB: 64 replicas: 0 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: gcp: osDisk: diskType: pd-ssd diskSizeGB: 64 replicas: 3 metadata: creationTimestamp: null name: okdlab networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 10.0.0.0/16 networkType: OVNKubernetes serviceNetwork: - 172.30.0.0/16 platform: gcp: projectID: michaeltcloud region: europe-west4 publish: External pullSecret: \u0026#39;{\u0026#34;auths\u0026#34;:{\u0026#34;....\u0026#39; sshKey: | ssh-rsa AAAAB3NzaC1yc..... This is the basic yaml file needed for the openshift-install to do it\u0026rsquo;s work. I will try to explain some things:\nbaseDomain directive The base domain directive is your DNS zone you created in GCP. In my case it is called gcp.alcatrash.net.\nreplicas: 0 directive in the worker section I have explicitely specified that there will be 0 workers. In that case, we only have a 3 node cluster with combined functions: etcd, controlPlane and worker capabilities. Ofcourse, this is not best practice, but for a lab environment it is perfect.\nmetadata.name directive This is the name of your okd cluster, and also will be part of the DNS. Every app will get a DNS record like appname.apps.okd.gcp.alcatrash.net in my case.\nplatform.gcp.region directive The region my virtual machines will be deployed.\npullSecret directive As mentioned in my previous blog a pull secret is needed for this to work. You can get your pull secret from the Red Hat site. Please check my previous blog for more details\nssh-key directive Paste your public ssh key in here. This can come in handy when ssh\u0026rsquo;ing into your nodes.\nRunning the installer After you created your own personal install-config.yaml, copy it into the install_dir directory:\n[michael@michael-nb2 okd_deploy]$ cp install-config.yaml install_dir/ After that, we can run the following command to create the manifests that the installer will use to configure the Openshift cluster:\n[michael@michael-nb2 okd_deploy]$ openshift-install create manifests --dir=install_dir/ INFO Credentials loaded from file \u0026#34;/home/michael/.gcp/osServiceAccount.json\u0026#34; INFO Consuming Install Config from target directory WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings INFO Manifests created in: install_dir/manifests and install_dir/openshift As you can see, the installer gives a warning that we don\u0026rsquo;t have any worker nodes, to the masters will be schedulable. This is fine, because it is a lab environment anyway.\nAfter that, we can create the ignition files. The ignition files are used for the Virtual machines. Ignition files contain information for CoreOS to know what to install and what to do.\n[michael@michael-nb2 okd_deploy]$ openshift-install create ignition-configs --dir=install_dir/ INFO Consuming Common Manifests from target directory INFO Consuming OpenShift Install (Manifests) from target directory INFO Consuming Openshift Manifests from target directory INFO Consuming Master Machines from target directory INFO Consuming Worker Machines from target directory INFO Ignition-Configs created in: install_dir and install_dir/auth The files are now created. Let\u0026rsquo;s see which files are created here:\n[michael@michael-nb2 okd_deploy]$ tree . ├── install-config.yaml └── install_dir ├── auth │ ├── kubeadmin-password │ └── kubeconfig ├── bootstrap.ign ├── master.ign ├── metadata.json └── worker.ign 2 directories, 7 files Now, it is time to create the cluster in GCP:\n[michael@michael-nb2 okd_deploy]$ openshift-install create cluster --dir=install_dir/ INFO Consuming Bootstrap Ignition Config from target directory INFO Consuming Master Ignition Config from target directory INFO Consuming Worker Ignition Config from target directory INFO Credentials loaded from file \u0026#34;/home/michael/.gcp/osServiceAccount.json\u0026#34; ... This will take anywhere between 20 - 40 minutes. In this time, you will see resources being created. I think the resources are created by terraform.\nAfter a while, the setup has ended.\n[michael@michael-nb2 okd_deploy]$ openshift-install create cluster --dir=install_dir/ INFO Consuming Bootstrap Ignition Config from target directory INFO Consuming Master Ignition Config from target directory INFO Consuming Worker Ignition Config from target directory INFO Credentials loaded from file \u0026#34;/home/michael/.gcp/osServiceAccount.json\u0026#34; INFO Creating infrastructure resources... INFO Waiting up to 20m0s (until 5:16PM) for the Kubernetes API at https://api.okdlab.gcp.alcatrash.net:6443... INFO API v1.24.0-2368+b62823b40c2cb1-dirty up INFO Waiting up to 30m0s (until 5:29PM) for bootstrapping to complete... INFO Destroying the bootstrap resources... INFO Waiting up to 40m0s (until 5:55PM) for the cluster at https://api.okdlab.gcp.alcatrash.net:6443 to initialize... INFO Waiting up to 10m0s (until 5:31PM) for the openshift-console route to be created... INFO Install complete! INFO To access the cluster as the system:admin user when using \u0026#39;oc\u0026#39;, run INFO export KUBECONFIG=/home/michael/okd_deploy/install_dir/auth/kubeconfig INFO Access the OpenShift web-console here: https://console-openshift-console.apps.okdlab.gcp.alcatrash.net INFO Login to the console with user: \u0026#34;kubeadmin\u0026#34;, and password: \u0026#34;xyz\u0026#34; INFO Time elapsed: 28m2s Done After clicking the console url you will encounter some self-signed certificate errors. Ignore these, becaust it is only a lab environment.\nafter logging in with the kubeadmin user we can see the shiny web interface of OKD.\nAnd there it is, your own 3 node OpenShift / OKD cluster. You can do everything with it. For example; i used it when studying for the DO280 exam.\nHave fun! And if you have any questions, please contact me through email / linkedin / github or whatever ;-)\n","permalink":"https://michaeltrip.nl/blog/deploy-okd-gcp/","tags":["blog","openshift","kubernetes","cloud"],"title":"Deploying OpenShift OKD in GCP (Google Cloud)"},{"categories":null,"contents":"Intro So i wanted to run OpenShift in my Homelab. Why? Because i can, and i had some spare time ;-).\nFor a long i wanted to setup a Openshift cluster in my own Homelab. Due to resource limitations i can only run one Openshift node. After many moons of searching and testing, i have found a very convinient way to enroll a single node OpenShift 4.11 (OKD) in my Homelab.\nPreperations First, there are some preperations to be done;\nA machine that can run podman, in my case i have a separate VM called okd4-services which hosts my podman pods. A virtual machine with the following specifications (this is what i have observed as the absolute minimum): 8 vCPU\u0026rsquo;s 24 GB (!!) of memory 100GB disk If possible, a DHCP reservation on the mac address of the VM. A dns zone that you can use in your own homelab, because OpenShift heavily relies on DNS. This is the dump of my DNS zone called okd.alcatrash.net. $ORIGIN . okd.alcatrash.net 3600 IN SOA ns1.alcatrash.org hostmaster.okd.alcatrash.net 2022090510 10800 3600 604800 3600 _etcd-server-ssl._tcp.okd.alcatrash.net 60 IN SRV 0 10 2380 etcd-0.okd.alcatrash.net api.okd.alcatrash.net 60 IN A 10.99.1.201 api-int.okd.alcatrash.net 60 IN A 10.99.1.201 *.apps.okd.alcatrash.net 60 IN A 10.99.1.201 console-openshift-console.apps.okd.alcatrash.net 60 IN A 10.99.1.201 oauth-openshift.apps.okd.alcatrash.net 60 IN A 10.99.1.201 console.okd.alcatrash.net 60 IN A 10.99.1.201 console-openshift-console.okd.alcatrash.net 60 IN A 10.99.1.201 etcd-0.okd.alcatrash.net 60 IN A 10.99.1.201 okd4-control-plane-1.okd.alcatrash.net 60 IN A 10.99.1.201 okd4-services.okd.alcatrash.net 60 IN A 10.99.1.210 As you can see we have several dns entries; 2 times a A record for the hosts okd4-services and the okd4-control-plane-1. The rest are mandatory DNS records needed for OpenShift to work properly. You can take a look here for more information.\nThe podman host must be reachable from the OpenShift node. Starting the OpenShift Assisted installer So first, login to your podman node. After that, clone the following:\n[michael@okd4-services ~]$ git clone https://github.com/openshift/assisted-service.git Cloning into \u0026#39;assisted-service\u0026#39;... remote: Enumerating objects: 60092, done. remote: Counting objects: 100% (24/24), done. remote: Compressing objects: 100% (23/23), done. remote: Total 60092 (delta 5), reused 10 (delta 0), pack-reused 60068 Receiving objects: 100% (60092/60092), 67.53 MiB | 8.06 MiB/s, done. Resolving deltas: 100% (41319/41319), done. Updating files: 100% (12157/12157), done. [michael@okd4-services ~]$ After that, cd into the assisted-service/deploy/podman directory.\nafter that, take a look in the directory:\n[michael@okd4-services podman]$ ls -lah total 44K drwxrwxr-x. 2 michael michael 185 Oct 5 21:38 . drwxrwxr-x. 12 michael michael 4.0K Oct 5 21:38 .. -rw-rw-r--. 1 michael michael 249 Oct 5 21:38 configmap_tls_certs.yml -rw-rw-r--. 1 michael michael 2.8K Oct 5 21:38 configmap_tls.yml -rw-rw-r--. 1 michael michael 4.6K Oct 5 21:38 configmap.yml -rw-rw-r--. 1 michael michael 1.8K Oct 5 21:38 okd-configmap.yml -rw-rw-r--. 1 michael michael 1.1K Oct 5 21:38 pod-persistent.yml -rw-rw-r--. 1 michael michael 1.6K Oct 5 21:38 pod_tls.yml -rw-rw-r--. 1 michael michael 810 Oct 5 21:38 pod.yml -rw-rw-r--. 1 michael michael 4.7K Oct 5 21:38 README.md Here is a file we need to edit.\nEdit the okd-configmap.yml and replace the ASSISTED_SERVICE_HOST, IMAGE_SERVICE_BASE_URL and the SERVICE_BASE_URL variables with the ip address of your host where you run the podman pods on.\nIn my case, it looks something like this:\n[michael@okd4-services podman]$ cat okd-configmap.yml apiVersion: v1 kind: ConfigMap metadata: name: config data: ASSISTED_SERVICE_HOST: 10.99.1.210:8090 IMAGE_SERVICE_BASE_URL: http://10.99.1.210:8888 SERVICE_BASE_URL: http://10.99.1.210:8090 ASSISTED_SERVICE_SCHEME: http ... After that, the README.md states we need to run the following command for the podman pods to start. podman play kube --configmap okd-configmap.yml pod.yml This run will be without data persistence. Should you require data persistence, than take a look in the README.md on how to proceed.\n[michael@okd4-services podman]$ podman play kube --configmap okd-configmap.yml pod.yml Trying to pull quay.io/centos7/postgresql-12-centos7:latest... Getting image source signatures .... Containers: 0f00fac2dfbcc183270210bb0a689ac3ddf9f22f2a98aa49a526f01368ba9a2e f00ff1daf4a5b869badec77c4931f1496d95eac86be29f6f06404d83ace5f6f4 75dc84c2d958f6716c66c449a7ec46a36ac556df2e60e6e155412b5741829a50 8b1a32f5055e85ba12a62b86c93a070f349906ddfe8cfc903771b5d37db8cedd After that, we can login into the assisted-service web interface by visiting the url where the pod was started. In my case this is http://10.99.1.210:8080.\nCreating a cluster in the Assisted service / installer After logging in on the web interface, you will see a nice interface:\nAssisted service Web UI click on Create New Cluster\nNow you will be shown a screen where you can setup the cluster. Make sure to enter the correct Cluster Name corresponding with your DNS zone. In my case my cluster name will be okd and my base domain will be alcatrash.net.\nAlso make sure to check the box Install single node OpenShift (SNO)\nAfter that, insert the pull secret for the Red Hat container registry. Your pull secret can be retrieved from the https://cloud.redhat.com/openshift/create/local url. Login with your Red Hat credentials. After that, you will be prompted with something like this:\nGet your Pull Secret here Click on Copy pull secret and paste it into the assisted installer screen.\nAfter that, you can choose for Host networking either DHCP or Static. In this example i have created a DHCP lease for my Openshift node in my router, so i will choose DHCP here.\nAfter that, click Next. After that, you will see the Operators section. We are not going to use any operators from here, so click Next.\nAfter clicking next, you will see the Host discovery screen. Click on the Add Host button. In this menu you can paste your SSH key. Insert your SSH key and choose the minimal image file. After that, you can click the Generate Discovery ISO Button. Now the magic will happen ;-).\nISO being generated At this point, the assisted installer generated a iso that can be used for provisioning the Openshift node.\nISO url Download the ISO by either copying the url or the wget command into your terminal and save the iso. Make sure to save it somewhere where you can mount it on your hypervisor.\nBooting your VM with the ISO At this point we are ready to boot the VM with the ISO attached. In this example i will use Proxmox, as this is my Hypervisor in my homelab.\nVM is booting The VM will boot and will call home to the assisted installer service with the provisioned agent on the ISO.\nagent calling home After some time the node will appear in the web ui from the assisted installer:\nServer appears in web ui After that, we can click Next. The following screen tells you your disks will be formatted.\nClick Next to continue. The following screen will have some network information. You can take a look at it, after that, hit Next.\nNow, you will be presented with the summary screen:\nsummary screen Click on Install cluster.\nAfter that, you can monitor your progress:\nMonitoring progress The complete installation will take up to 60 minutes on modern hardware. Grab a cup of coffee and wait some time ;-)\nAfter waiting, you will be presented with this screen:\nInstallation completed You will see some information on your screen including the console url and the credentials for the kubeadmin user.\nDone, what now ? After clicking the console url you will encounter some self-signed certificate errors. Ignore these, because it is your homelab ;-).\nafter logging in with the kubeadmin user we can see the shiny web interface of OKD.\nOKD Web UI And there it is, your own single node OpenShift / OKD cluster. You can do everything with it, except one thing: upgrading to a new release. That is not possible.\nHave fun! And if you have any questions, please contact me through email / linkedin / github or whatever ;-)\n","permalink":"https://michaeltrip.nl/blog/openshift-sno-in-homelab/","tags":["blog","openshift","kubernetes","homelab"],"title":"Setting up Openshift single node in your own Homelab"},{"categories":null,"contents":"Intro So i passed the CKAD exam last september. In this blog i wanted to share some tips.\nLearning matters For me personally i am the type of guy that learns by just doing it. Set up some Kubernetes node with either Kind or Rancher Desktop and just try some things out.\nAlso, check out some great Udemy courses. I personally liked this CKAD course by Mumshad Mannambeth.\nAlso, try some practice exercises from the internet. I just looked some of those up on Github:\nhttps://github.com/dgkanatsios/CKAD-exercises https://github.com/jamesbuckett/ckad-questions Also, if you have booked the exam, you are also entitled to use the Exam simulator by killer.sh. If you log-in on the site of the Linux Foundation there is a link to that exam simulator. You have 2 shots.\nUse autocompletion in your bashrc When you are working on the CKAD exam, speed is everything. You have approx 15-17 questions to do within two hours, so time is crucial.\nYou can speed things up by using auto-completion in your shell. To do so, enable auto completion for kubectl in your .bashrc.\necho \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # add autocomplete permanently to your bash shell. After that, you can hit the button on your keyboard to autocomplete commands. If you are tired of of typing in the whole kubectl command you can also alias that command.\necho \u0026#34;alias k=kubectl\u0026#34; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#34;complete -F __start_kubectl k\u0026#34; \u0026gt;\u0026gt; ~/.bashrc More information can be found in the Kubectl cheatsheet, which you can also use on the exam.\nUse bookmarks in your browser The exam you are about to take is a web-based exam. You are allowed to have one tab open besides your exam tab. This can either be one of the following sites:\nhttps://kubernetes.io/docs/ https://github.com/kubernetes/ https://kubernetes.io/blog/ To make things easier, you are allowed to use bookmarks in your browser. I have created some bookmarks that you can import in your browser. You can find them here.\nUse kubectl expose This is a odd one, but i found it very usable. When you create a pod or a deployment you want it to be served some how. To do this, you can quickly create a service for this pod or deployment.\nFor example: I have a pod called nginx. To create a service for it, i can use kubectl expose.\nkubectl expose pod nginx --port=80 --name=servicename Or, if you want a yaml file to edit it and create some more service parameters, use the following command:\nkubectl expose pod nginx --port --name=servicename --dry-run=client -o yaml \u0026gt; service.yml Use a alias to quickly switch to another namespace Exam quests will take place in different namespaces and contexts. If there is a question that doesn´t mention a namespace, always use the default namespace.\nYou can switch namespaces by typing this command:\nkubectl config set-context --current --namespace namespacename A bit long, isn´t it? To make life easier i use a alias you can add to your .bashrc.\necho \u0026#39;alias kn=\u0026#34;kubectl config set-context --current --namespace\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc Create your own .vimrc If you use vim as your favorite editor, it is important to use the right configuration. You can edit your .vimrc in your home folder. I used the following settings:\nset nu # set numbers set tabstop=2 shiftwidth=2 expandtab # use 2 spaces instead of tab set ai # autoindent: when go to new line keep same indentation set cursorcolumn # You see a column on your cursor so you can easily check the identation. Some other tips Stay hydrated You are allowed to have a glass or a bottle of water next to you. Mind that this bottle or glass needs to be transparent.\nClean up your desk before taking the exam I started the exam 15 minutes before and cleaned up my desk. But, i left my audio speakers on the desk. The proctor asked me to also remove these audio speakers. So please, clean up your desk and remove everything from your desk besides your keyboard, mouse and your laptop.\nCheck your questions before you begin Go through all the questions and begin with the easiest ones first. After that, you have plenty of time to start with the hard questions.\nCheck in which context you have to use At the beginning of every question, it states in which context you will work in. There is also a copy button with to command that you can use. Use this button, it will save you a lot of time.\n","permalink":"https://michaeltrip.nl/blog/setting-up-kubernetes/","tags":["kubernetes","Homelab"],"title":"Tips on the CKAD exam"},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip; is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"https://michaeltrip.nl/projects/creations/bosh-agents/","tags":["DevOps","BOSH","Java","Atlassian Ecosystem","monit","python","xml/xslt","bash/shell","REST APIs"],"title":"BOSH release for Bamboo \u0026 Remote Agents"},{"categories":null,"contents":"Multiple plugins used by thousands of teams that provide enhanced functionality of Atlassian’s core products (primarily JIRA and Bamboo) to enrich CI/CD capabilities, DevOps automation, or productivity. Functionality spans user interface, web services and persistence.\n","permalink":"https://michaeltrip.nl/projects/creations/marketplace/","tags":["Java","Spring","REST APIs","Javascript","Atlassian Developer Ecosystem","Bamboo","JIRA","Bitbucket","Confluence","DevOps"],"title":"Atlassian Marketplace Plugins"},{"categories":null,"contents":"Provides required dependencies and additional utilities to simplify and codify the process of building, testing and delivering Atlassian plugins all the way to the live marketplace. Executes integration/AUT level tests against all stated compatible versions for the productUploads generated artifact to Atlassian marketplaceProvides corresponding metadata indicating version, release notes, and compatibility\n","permalink":"https://michaeltrip.nl/projects/creations/docker-marketplace/","tags":["Docker","Maven","Java","Python","REST APIs","Bash/Shell"],"title":"Docker image for Bitbucket CI/CD Pipelines  \"shipit\""},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://michaeltrip.nl/search/","tags":null,"title":"Search Results"}]